{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import pandas as pd\n",
    "from subprocess import PIPE, Popen\n",
    "from glob import glob\n",
    "from datetime import timedelta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Corporacion Favorita"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#A sales forcasting problem for the Ecuadorian Grocery chain Corporacion Favorita\n",
    "#https://www.kaggle.com/c/favorita-grocery-sales-forecasting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's take a look at the competition data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['median_ma8.csv.gz',\n",
       " 'oil.csv',\n",
       " 'test.csv',\n",
       " 'sample_submission.csv',\n",
       " 'stores.csv',\n",
       " 'items.csv',\n",
       " 'train.csv',\n",
       " 'holidays_events.csv',\n",
       " 'transactions.csv']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATA_DIR = '../data/'\n",
    "os.listdir(DATA_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect files by commandline rather than loading with pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####################  Head of file: ../data/holidays_events.csv\n",
      "date,type,locale,locale_name,description,transferred\n",
      "2012-03-02,Holiday,Local,Manta,Fundacion de Manta,False\n",
      "2012-04-01,Holiday,Regional,Cotopaxi,Provincializacion de Cotopaxi,False\n",
      "2012-04-12,Holiday,Local,Cuenca,Fundacion de Cuenca,False\n",
      "2012-04-14,Holiday,Local,Libertad,Cantonizacion de Libertad,False\n",
      "2012-04-21,Holiday,Local,Riobamba,Cantonizacion de Riobamba,False\n",
      "2012-05-12,Holiday,Local,Puyo,Cantonizacion del Puyo,False\n",
      "2012-06-23,Holiday,Local,Guaranda,Cantonizacion de Guaranda,False\n",
      "2012-06-25,Holiday,Regional,Imbabura,Provincializacion de Imbabura,False\n",
      "2012-06-25,Holiday,Local,Latacunga,Cantonizacion de Latacunga,False\n",
      "\n",
      "####################  Head of file: ../data/items.csv\n",
      "item_nbr,family,class,perishable\n",
      "96995,GROCERY I,1093,0\n",
      "99197,GROCERY I,1067,0\n",
      "103501,CLEANING,3008,0\n",
      "103520,GROCERY I,1028,0\n",
      "103665,BREAD/BAKERY,2712,1\n",
      "105574,GROCERY I,1045,0\n",
      "105575,GROCERY I,1045,0\n",
      "105576,GROCERY I,1045,0\n",
      "105577,GROCERY I,1045,0\n",
      "\n",
      "####################  Head of file: ../data/oil.csv\n",
      "date,dcoilwtico\n",
      "2013-01-01,\n",
      "2013-01-02,93.14\n",
      "2013-01-03,92.97\n",
      "2013-01-04,93.12\n",
      "2013-01-07,93.2\n",
      "2013-01-08,93.21\n",
      "2013-01-09,93.08\n",
      "2013-01-10,93.81\n",
      "2013-01-11,93.6\n",
      "\n",
      "####################  Head of file: ../data/sample_submission.csv\n",
      "id,unit_sales\n",
      "125497040,0\n",
      "125497041,0\n",
      "125497042,0\n",
      "125497043,0\n",
      "125497044,0\n",
      "125497045,0\n",
      "125497046,0\n",
      "125497047,0\n",
      "125497048,0\n",
      "\n",
      "####################  Head of file: ../data/stores.csv\n",
      "store_nbr,city,state,type,cluster\n",
      "1,Quito,Pichincha,D,13\n",
      "2,Quito,Pichincha,D,13\n",
      "3,Quito,Pichincha,D,8\n",
      "4,Quito,Pichincha,D,9\n",
      "5,Santo Domingo,Santo Domingo de los Tsachilas,D,4\n",
      "6,Quito,Pichincha,D,13\n",
      "7,Quito,Pichincha,D,8\n",
      "8,Quito,Pichincha,D,8\n",
      "9,Quito,Pichincha,B,6\n",
      "\n",
      "####################  Head of file: ../data/test.csv\n",
      "id,date,store_nbr,item_nbr,onpromotion\n",
      "125497040,2017-08-16,1,96995,False\n",
      "125497041,2017-08-16,1,99197,False\n",
      "125497042,2017-08-16,1,103501,False\n",
      "125497043,2017-08-16,1,103520,False\n",
      "125497044,2017-08-16,1,103665,False\n",
      "125497045,2017-08-16,1,105574,False\n",
      "125497046,2017-08-16,1,105575,False\n",
      "125497047,2017-08-16,1,105576,False\n",
      "125497048,2017-08-16,1,105577,False\n",
      "\n",
      "####################  Head of file: ../data/train.csv\n",
      "id,date,store_nbr,item_nbr,unit_sales,onpromotion\n",
      "0,2013-01-01,25,103665,7.0,\n",
      "1,2013-01-01,25,105574,1.0,\n",
      "2,2013-01-01,25,105575,2.0,\n",
      "3,2013-01-01,25,108079,1.0,\n",
      "4,2013-01-01,25,108701,1.0,\n",
      "5,2013-01-01,25,108786,3.0,\n",
      "6,2013-01-01,25,108797,1.0,\n",
      "7,2013-01-01,25,108952,1.0,\n",
      "8,2013-01-01,25,111397,13.0,\n",
      "\n",
      "####################  Head of file: ../data/transactions.csv\n",
      "date,store_nbr,transactions\n",
      "2013-01-01,25,770\n",
      "2013-01-02,1,2111\n",
      "2013-01-02,2,2358\n",
      "2013-01-02,3,3487\n",
      "2013-01-02,4,1922\n",
      "2013-01-02,5,1903\n",
      "2013-01-02,6,2143\n",
      "2013-01-02,7,1874\n",
      "2013-01-02,8,3250\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for fl in sorted(glob(DATA_DIR + '*.csv')):\n",
    "    p = Popen(['head', DATA_DIR + '{}'.format(fl)], stdin=PIPE, stdout=PIPE, stderr=PIPE)\n",
    "    output, err = p.communicate()\n",
    "    print('#'*20 + '  Head of file: {}'.format(fl))\n",
    "    output = output.decode('ascii')\n",
    "    for ll in output.split('\\n'):\n",
    "        print(ll)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](schema.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Above from: https://www.kaggle.com/jeru666/all-csv-files-a-glance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Under the hypotheses that grocery sales volumes are impacted by Ecuadorian holidays and oil prices,\n",
    "# files with this information were included. \n",
    "# We are also given information about an Earthquake and that employees are typically paid on the 15th\n",
    "# and the last day of the month.\n",
    "# This tabular dataset of contextualized data fields is amenable to exploring feature engineering \n",
    "# and common-sense heuristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The sample_submission file shows that for each store id, we want to predict unit_sales\n",
    "# Note the evaluation metric penalizes error in perishable items more severely\n",
    "# https://www.kaggle.com/c/favorita-grocery-sales-forecasting#evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load train.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/funk/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2717: DtypeWarning: Columns (5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv(DATA_DIR + 'train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>store_nbr</th>\n",
       "      <th>item_nbr</th>\n",
       "      <th>unit_sales</th>\n",
       "      <th>onpromotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>25</td>\n",
       "      <td>103665</td>\n",
       "      <td>7.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>25</td>\n",
       "      <td>105574</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>25</td>\n",
       "      <td>105575</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>25</td>\n",
       "      <td>108079</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>25</td>\n",
       "      <td>108701</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>25</td>\n",
       "      <td>108786</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>25</td>\n",
       "      <td>108797</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>25</td>\n",
       "      <td>108952</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>25</td>\n",
       "      <td>111397</td>\n",
       "      <td>13.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>25</td>\n",
       "      <td>114790</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>25</td>\n",
       "      <td>114800</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>25</td>\n",
       "      <td>115267</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>25</td>\n",
       "      <td>115611</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>25</td>\n",
       "      <td>115693</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>25</td>\n",
       "      <td>115720</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>25</td>\n",
       "      <td>115850</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>25</td>\n",
       "      <td>115891</td>\n",
       "      <td>6.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>25</td>\n",
       "      <td>115892</td>\n",
       "      <td>10.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18</td>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>25</td>\n",
       "      <td>115894</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19</td>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>25</td>\n",
       "      <td>119024</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    id        date  store_nbr  item_nbr  unit_sales onpromotion\n",
       "0    0  2013-01-01         25    103665         7.0         NaN\n",
       "1    1  2013-01-01         25    105574         1.0         NaN\n",
       "2    2  2013-01-01         25    105575         2.0         NaN\n",
       "3    3  2013-01-01         25    108079         1.0         NaN\n",
       "4    4  2013-01-01         25    108701         1.0         NaN\n",
       "5    5  2013-01-01         25    108786         3.0         NaN\n",
       "6    6  2013-01-01         25    108797         1.0         NaN\n",
       "7    7  2013-01-01         25    108952         1.0         NaN\n",
       "8    8  2013-01-01         25    111397        13.0         NaN\n",
       "9    9  2013-01-01         25    114790         3.0         NaN\n",
       "10  10  2013-01-01         25    114800         1.0         NaN\n",
       "11  11  2013-01-01         25    115267         1.0         NaN\n",
       "12  12  2013-01-01         25    115611         1.0         NaN\n",
       "13  13  2013-01-01         25    115693         1.0         NaN\n",
       "14  14  2013-01-01         25    115720         5.0         NaN\n",
       "15  15  2013-01-01         25    115850         1.0         NaN\n",
       "16  16  2013-01-01         25    115891         6.0         NaN\n",
       "17  17  2013-01-01         25    115892        10.0         NaN\n",
       "18  18  2013-01-01         25    115894         5.0         NaN\n",
       "19  19  2013-01-01         25    119024         1.0         NaN"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017 instances begin at index 101688779\n"
     ]
    }
   ],
   "source": [
    "if train.date.is_monotonic_increasing:\n",
    "    print('2017 instances begin at index {}'.format(len(train.query(\"date < '2017-01-01'\"))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "unit_sales_stats = dict(train['unit_sales'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'25%': 2.0,\n",
       " '50%': 4.0,\n",
       " '75%': 9.0,\n",
       " 'count': 125497040.0,\n",
       " 'max': 89440.0,\n",
       " 'mean': 8.5548652684376716,\n",
       " 'min': -15372.0,\n",
       " 'std': 23.605151805092405}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unit_sales_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Above, we find that unit sales vary across several orders of magnitude suggesting the log transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple baseline using the median of moving averages over several time scales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# based on: https://www.kaggle.com/paulorzp/log-moving-averages-forecasting-lb-0-546/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtypes = {'id':'int64', 'item_nbr':'int32', 'store_nbr':'int8'}       # use less memory, csv is 4.7 GB\n",
    "converters = {'unit_sales':lambda u: float(u) if float(u)>0 else 0}   # coerce negatives to 0\n",
    "data_pre_2017 = range(1, 101688779)                                   # keep header, drop data prior to 2017   \n",
    "train = pd.read_csv(DATA_DIR + 'train.csv', usecols=[1,2,3,4], \n",
    "                    dtype=dtypes, parse_dates=['date'],\n",
    "                    converters=converters,\n",
    "                    skiprows=data_pre_2017\n",
    "                   )\n",
    "train['unit_sales'] =  train['unit_sales'].apply(pd.np.log1p)         # Apply log1p for RMSLE, NOTE: IGNORES WEIGHTS\n",
    "u_dates = train.date.unique()\n",
    "u_stores = train.store_nbr.unique()\n",
    "u_items = train.item_nbr.unique()\n",
    "train.set_index(['date', 'store_nbr', 'item_nbr'], inplace=True)\n",
    "train = train.reindex(\n",
    "    pd.MultiIndex.from_product(\n",
    "        (u_dates, u_stores, u_items),\n",
    "        names=['date', 'store_nbr', 'item_nbr']\n",
    "    )   \n",
    ")\n",
    "\n",
    "del u_dates, u_stores, u_items                                         # release memory, kaggle kernel\n",
    "\n",
    "train.loc[:, 'unit_sales'].fillna(0, inplace=True)                     # replace NaNs\n",
    "train.reset_index(inplace=True)                                        # reset index and restoring unique columns  \n",
    "lastdate = train.iloc[train.shape[0]-1].date\n",
    "\n",
    "test = pd.read_csv(DATA_DIR + 'test.csv', usecols=[0,2,3], dtype=dtypes)\n",
    "test = test.set_index(['item_nbr', 'store_nbr'])\n",
    "ltest = test.shape[0]\n",
    "\n",
    "MA_windows = [1,3,7,14,28,56,112,224]\n",
    "#Moving Averages\n",
    "for i in MA_windows:\n",
    "    val='MA'+str(i)\n",
    "    tmp = train[train.date>lastdate-timedelta(int(i))]\n",
    "    tmp1 = tmp.groupby(['item_nbr', 'store_nbr'])['unit_sales'].mean().to_frame(val)\n",
    "    test = test.join(tmp1, how='left')\n",
    "\n",
    "#Median of MAs\n",
    "test['unit_sales']=test.iloc[:,1:].median(axis=1)\n",
    "test.loc[:, 'unit_sales'].fillna(0, inplace=True)\n",
    "test['unit_sales'] = test['unit_sales'].apply(pd.np.expm1)             # restoring unit values \n",
    "test[['id','unit_sales']].to_csv(DATA_DIR + 'median_ma8.csv.gz', index=False, float_format='%.3f', \n",
    "                                 compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Submission scores approximately 0.546 on the public leaderboard, 2 mo. since launch scoring 50th percentile!\n",
    "# Surprisingly good performance considering this model uses simple statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This model assumes:\n",
    "*  Recent unit_sales are sufficient\n",
    "*  is_perishable weighting is negligible\n",
    "*  moving avg over various time scales suffices to make prediction\n",
    "*  distributional skew makes the more robust median statistic appropriate for aggregation\n",
    "*  other data sources are less informative\n",
    "*  information loss in coercing negatives to 0 of minor impact"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further Directions:\n",
    "*  Introduce weighting for the RMSLWE\n",
    "*  Explore relationship between oil prices, day-of-month, is_holiday, onpromotion and unit_sales\n",
    "*  Cluster stores, assumes greater statistical efficiency\n",
    "*  Explore tree-based models\n",
    "*  Use more training data\n",
    "*  Generalize to a function of dataframe, filters, aggregators"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
